{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06c78fb4-ddca-465d-845b-6af84b3378d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import logging\n",
    "import os\n",
    "import awswrangler as wr\n",
    "import redshift_connector\n",
    "\n",
    "# --- S3 and Redshift config ---\n",
    "S3_BUCKET = 'XXX'\n",
    "FILENAME='DimReseller.csv'\n",
    "S3_PATH = f's3://{S3_BUCKET}/{FILENAME}'\n",
    "AWS_REGION = 'XXX'\n",
    "IAM_ROLE = 'XXX'  # Role with S3 read access\n",
    "\n",
    "# --- File and Table ---\n",
    "EXCEL_PATH = 'AWSCloud/Adventure Works on AWs/Entities used in Dashboard/DimReseller.csv'\n",
    "TARGET_TABLE = 'DimReseller'\n",
    "STAGING_TABLE = 'Reseller'\n",
    "\n",
    "\n",
    "# --- Setup logging ---\n",
    "logging.basicConfig(filename='D:/person.log',\n",
    "                    level=logging.INFO,\n",
    "                    #format='%(asctime)s %(levelname)s %(lineno)d : %(message)s')\n",
    "                    format='%(asctime)s - %(name)s - %(levelname)s - %(filename)s:%(lineno)d - %(message)s')\n",
    "\n",
    "# --- Redshift credentials ---\n",
    "REDSHIFT_CONFIG = {\n",
    "    'database': 'dev',\n",
    "    'user': 'admin',\n",
    "    'password': 'XXX',\n",
    "    'host': 'XXX',\n",
    "    'port': 'XXX' \n",
    "}\n",
    "\n",
    "\n",
    "# --- Connect to Redshift ---\n",
    "def get_redshift_connection():\n",
    "    try:\n",
    "        conn = redshift_connector.connect(**REDSHIFT_CONFIG)\n",
    "        conn.autocommit = False\n",
    "        return conn\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Redshift connection failed: {e}\")\n",
    "        raise\n",
    "\n",
    "# --- Load Excel & drop duplicates ---\n",
    "def load_and_clean_excel(path):\n",
    "    try:\n",
    "        df = pd.read_csv(path)\n",
    "        logging.info(f\"Loaded Excel with {len(df)} rows.\")\n",
    "        \n",
    "        df.drop_duplicates(subset=['ResellerKey'], keep='last', inplace=True)\n",
    "        logging.info(f\"Removed duplicates, remaining rows: {len(df)}.\")\n",
    "        \n",
    "        return df\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Excel load/clean failed: {e}\")\n",
    "        raise\n",
    "\n",
    "# --- Upload DataFrame to S3 and load into Redshift using COPY ---\n",
    "def upload_to_staging(conn, df):\n",
    "    try:\n",
    "        # Upload CSV to S3 using awswrangler\n",
    "        wr.s3.to_csv(df,S3_PATH,index=False)\n",
    "        \n",
    "        logging.info(f\"CSV written to S3 at {S3_PATH}\")\n",
    "\n",
    "        # Run COPY command in Redshift\n",
    "        cursor = conn.cursor()\n",
    "        cursor.execute(f\"DELETE FROM {STAGING_TABLE}\")\n",
    "\n",
    "        copy_sql = f\"\"\"\n",
    "        COPY {STAGING_TABLE} (ResellerKey,ResellerName,ProductLine,GeographyKey)\n",
    "        FROM '{S3_PATH}'\n",
    "        IAM_ROLE '{IAM_ROLE}'\n",
    "        REGION '{AWS_REGION}'\n",
    "        FORMAT AS CSV\n",
    "        TIMEFORMAT AS 'MM/DD/YYYY HH:MI'\n",
    "        ENCODING UTF8\n",
    "        IGNOREHEADER 1\n",
    "        EMPTYASNULL\n",
    "        BLANKSASNULL\n",
    "        NULL AS ''\n",
    "        \"\"\"\n",
    "        cursor.execute(copy_sql)\n",
    "        logging.info(\"COPY from S3 to staging table completed.\")\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Failed in upload_to_staging: {e}\")\n",
    "        raise\n",
    "\n",
    "\n",
    "# --- Merge logic to prevent duplicates ---\n",
    "def merge_data(conn):\n",
    "    cursor = conn.cursor()\n",
    "    try:\n",
    "        merge_sql = f\"\"\"\n",
    "        MERGE INTO {TARGET_TABLE} \n",
    "        USING {STAGING_TABLE} \n",
    "        ON {TARGET_TABLE}.resellerkey = {STAGING_TABLE}.resellerkey\n",
    "        WHEN MATCHED THEN\n",
    "            UPDATE SET\n",
    "                resellername={STAGING_TABLE}.resellername,\n",
    "                ProductLine={STAGING_TABLE}.ProductLine,   \n",
    "                GeographyKey={STAGING_TABLE}.GeographyKey \n",
    "                \n",
    "        WHEN NOT MATCHED THEN\n",
    "            INSERT (ResellerKey,ResellerName,ProductLine,GeographyKey)\n",
    "            VALUES ({STAGING_TABLE}.resellerkey, {STAGING_TABLE}.resellername,{STAGING_TABLE}.ProductLine,{STAGING_TABLE}.GeographyKey)          \n",
    "        \"\"\"\n",
    "        cursor.execute(merge_sql)\n",
    "        logging.info(\"Merge completed successfully.\")\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Merge failed: {e}\")\n",
    "        raise\n",
    "\n",
    "# --- Main pipeline ---\n",
    "def main():\n",
    "    try:\n",
    "        print(os.getcwd())\n",
    "        df = load_and_clean_excel(EXCEL_PATH)\n",
    "        #clean_excel(df)\n",
    "        conn = get_redshift_connection()\n",
    "        #upload_to_redshift(conn, df, STAGING_TABLE)\n",
    "        upload_to_staging(conn,df)\n",
    "        merge_data(conn)\n",
    "        conn.commit()\n",
    "        logging.info(\"Pipeline finished successfully.\")\n",
    "    except Exception as e:\n",
    "        if 'conn' in locals():\n",
    "            conn.rollback()\n",
    "            logging.error(\"Rolled back due to failure.\")\n",
    "        logging.error(f\"Pipeline failed: {e}\")\n",
    "    finally:\n",
    "        if 'conn' in locals():\n",
    "            conn.close()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
